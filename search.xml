<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Deep Reinforcement Learning - Overview</title>
    <url>/2022/10/26/test/</url>
    <content><![CDATA[<h1 id="Deep-Reinforcement-Learning-Overview"><a href="#Deep-Reinforcement-Learning-Overview" class="headerlink" title="Deep Reinforcement Learning - Overview"></a>Deep Reinforcement Learning - Overview</h1><h2 id="1-RL-Basics"><a href="#1-RL-Basics" class="headerlink" title="1.RL Basics"></a>1.RL Basics</h2><h3 id="1-1-Terminology"><a href="#1-1-Terminology" class="headerlink" title="1.1 Terminology"></a>1.1 Terminology</h3><ul>
<li>State：当前环境的状态空间</li>
<li>Action：Agent当前可以采取的动作空间</li>
<li>Policy $\pi$ ：policy函数$\pi:(s,a) -&gt; [0,1]$ $\pi:(a | s)=P(A=a|S=s)$ ，大写字母代表还没有观测到的随机变量，小写字母代表已经观测到的确定值。策略函数做的事情就是：在给定状态s下，agent会采取不同action的概率。</li>
</ul>
<p>以超级玛丽的游戏为例，假设你正在玩超级玛丽，游戏某一时刻的截图如下：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042114190.png" style="zoom: 33%;" /></p>
<p>此时我们假设观测到的画面observstion就是当前的状态state(虽然情况可能并没有这么简单)，那么现在state有了。Action是啥？</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042116087.png" style="zoom: 33%;" /></p>
<p>Action就是身为Agent的玛丽当前三个可选择的动作，{left, right, up}。</p>
<p>假设现在有一个策略函数$\pi$，根据该函数可以得到agent采取不同动作的概率：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042119431.png" style="zoom: 33%;" /></p>
<ul>
<li>Reward：奖励是自定义的，根据agent的状态定义不同的reward。比如：</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042122133.png" style="zoom: 33%;" /></p>
<ul>
<li>State Transition：状态转移，在一个old state，采取了某一个action，得到了一个new state。状态转移也可以是随机的，比如说在某一个state采取了某一个action，那么下一个state也可能是随机的。$p(s^{‘}|s,a)=P(S^{‘}=s^{‘}|S=s,A=a)$</li>
</ul>
<h3 id="1-2-Two-Sources-of-Randomness"><a href="#1-2-Two-Sources-of-Randomness" class="headerlink" title="1.2 Two Sources of Randomness"></a>1.2 Two Sources of Randomness</h3><ul>
<li>第一种是action的随机性，即处于某一个状态时，agent会采取的action是随机的，也就是根据策略函数来的：</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042232132.png" style="zoom:33%;" /></p>
<ul>
<li>第二种是state的随机性，即处于某一个状态，采取某个action后，下一个新的state是随机的，也就是根据状态转移函数来的：</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042236648.png" style="zoom:33%;" /></p>
<h3 id="1-3-Agent-Environment-Interaction"><a href="#1-3-Agent-Environment-Interaction" class="headerlink" title="1.3 Agent-Environment Interaction"></a>1.3 Agent-Environment Interaction</h3><p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042237031.png" style="zoom:33%;" /></p>
<h3 id="1-4-Rewards-and-Returns"><a href="#1-4-Rewards-and-Returns" class="headerlink" title="1.4 Rewards and Returns"></a>1.4 Rewards and Returns</h3><ul>
<li>Return：从当前时刻t开始以后的Reward之和。</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042240965.png" style="zoom:33%;" /></p>
<p>那么问题是，当前时刻的Reward和以后时刻的Reward并不是相同重要的，所以要做Discount。</p>
<ul>
<li>Discounted Return：</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042244036.png" style="zoom:33%;" /></p>
<p>$\gamma$ 是Discount factor，一个可调节超参数，属于$[0,1]$。</p>
<p>注意到，$U_{t}$跟以后所有时刻的Reward有关，所以只有当整个游戏或者说交互过程结束后，我们才能计算$U_t$的值。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042248048.png" style="zoom:33%;" /></p>
<p>由于在时刻t，$R_t…R_n$是随机的，所以$U_t$也是随机的。</p>
<p>并且$R_i$取决于$S_i、A_i$，因此$U_t$取决于$R_t…R_n$以及$A_i…A_n$。</p>
<h3 id="1-5-Value-Function"><a href="#1-5-Value-Function" class="headerlink" title="1.5 Value Function"></a>1.5 Value Function</h3><h4 id="1-5-1-Action-value-function-Q-pi-s-a"><a href="#1-5-1-Action-value-function-Q-pi-s-a" class="headerlink" title="1.5.1 Action value function $Q_{\pi}(s,a)$"></a>1.5.1 Action value function $Q_{\pi}(s,a)$</h4><p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042258491.png" style="zoom:33%;" /></p>
<p>$Q_{\pi}(s,a)$是对随机变量$U_t$求期望求出来的，所以是一个值。</p>
<h4 id="1-5-2-State-Value-Function-V-pi-s"><a href="#1-5-2-State-Value-Function-V-pi-s" class="headerlink" title="1.5.2 State Value Function $V_{\pi}(s)$"></a>1.5.2 State Value Function $V_{\pi}(s)$</h4><p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042303189.png" style="zoom:33%;" /></p>
<p>对于 $Q_{\pi}(s,a)$函数，确定state为$s_t$的情况下，就变成了$Q_{\pi}(s,A)$，是一个随机变量，因此可以对其求期望，就得到了$V_{\pi}(s_t)$。</p>
<ul>
<li>如果action是离散的，求期望就是用概率乘以随机变量的值再求和。</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042306730.png" style="zoom:33%;" /></p>
<ul>
<li>如果action是连续的，求期望就是用概率乘以随机变量的值再积分。</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042309483.png" style="zoom:33%;" /></p>
<h4 id="1-5-3-Understanding-of-Value-Functions"><a href="#1-5-3-Understanding-of-Value-Functions" class="headerlink" title="1.5.3 Understanding of Value Functions"></a>1.5.3 Understanding of Value Functions</h4><ul>
<li>对于Action Value Function，$Q_{\pi}(s,a)$在状态s时，agent采取行动a的好坏，值越高代表，采取这个action越好。</li>
<li>对于 State Value Function，对于一个确定的策略函数$\pi$，$V_{\pi}(s)$评价了当前agent处于状态s的这个环境好坏。</li>
</ul>
<h2 id="2-Value-Based-RL"><a href="#2-Value-Based-RL" class="headerlink" title="2. Value Based RL"></a>2. Value Based RL</h2><h4 id="2-1-Action-Value-Functions"><a href="#2-1-Action-Value-Functions" class="headerlink" title="2.1 Action-Value Functions"></a>2.1 Action-Value Functions</h4><p>对于在某一个策略$\pi$下的$Q(s,a)$，$Q^*(s_t,a_t)$的定义为，找到一个$\pi$，使得$Q$函数值最大。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209060909413.png" style="zoom:33%;" /></p>
<h4 id="2-2-Deep-Q-Network-DQN"><a href="#2-2-Deep-Q-Network-DQN" class="headerlink" title="2.2 Deep Q-Network(DQN)"></a>2.2 Deep Q-Network(DQN)</h4><p>以超级玛丽游戏为例，假设我们知道了$Q^<em>$函数，那么如何通过这个函数来玩游戏，采取最佳的策略呢？很显然，我们要采取能够使$Q^</em>$函数值最大的那个action $a^*$。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209060914336.png" style="zoom:33%;" /></p>
<p>这样，在$Q^<em>$函数的指导下，我们就可以知道在一个state s下该采取什么action了。但问题在于，我们并不知道$Q^</em>$函数，如何解决？</p>
<p>Deep Q Network(DQN)就派上用场了，用神经网络的方法来模拟函数$Q^<em>$，即用$Q(s,a;w)$来模拟$Q^</em>(s,a)$。</p>
<p>对于DQN来说：</p>
<ul>
<li>输入：当前的状态</li>
<li>输出：一个维度是动作空间的向量，代表每一个动作的分数</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209060920619.png" style="zoom:33%;" /></p>
<p>根据模拟出来的结果，采取对应分数最高的action，即“up”。</p>
<p>整个流程是这个亚子：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209060921261.png" style="zoom:33%;" /></p>
<h4 id="2-3-Temporal-Difference-TD-Learning"><a href="#2-3-Temporal-Difference-TD-Learning" class="headerlink" title="2.3 Temporal Difference(TD) Learning"></a>2.3 Temporal Difference(TD) Learning</h4><p>将一个大的模型估计，拆分成两部分，一部分是实际观测值，另一部分是小的模型估计：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209060947066.png" style="zoom:33%;" /></p>
<p>在RL中，类似的采用如下方式使用TD算法：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061020537.png" style="zoom:33%;" /></p>
<p>简单证明下：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061021882.png" style="zoom:33%;" /></p>
<p>得到：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061021070.png" style="zoom:33%;" /></p>
<ul>
<li>$Q(s_t,a_t;w)$，是对$U_t$的估计</li>
<li>$Q(s_{t+1},a_{t+1};w)$，是对$U_{t+1}$的估计</li>
</ul>
<p>因此</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061104099.png" style="zoom:33%;" /></p>
<p>TD Target：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061106530.png" style="zoom:33%;" /></p>
<p>Loss：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061107169.png" style="zoom:33%;" /></p>
<p>Gradient descent：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061108858.png" style="zoom:33%;" /></p>
<p>一次TD算法的迭代：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061110900.png" style="zoom:33%;" /></p>
<h2 id="3-Policy-Based-RL"><a href="#3-Policy-Based-RL" class="headerlink" title="3.Policy-Based RL"></a>3.Policy-Based RL</h2><p>对于策略函数$\pi(a|s)$，给出的函数值是一个向量，代表在当前state s下，agent采取不同action的概率。那么如果有一个好的$\pi$，我们就可以根据$\pi$来玩游戏了。但是如何去学习到一个好的$\pi$呢？</p>
<p>当然，对于有限的state和action，最简单的方式是枚举。通过玩很多次游戏，将所有state和action的组合做一个表格，这样就得到了一个$\pi$。但是对于state和action的间较大的情况，这种方法不适用，因此我们采用Policy Network的方法。</p>
<h3 id="3-1-Policy-Network-pi-a-s-theta"><a href="#3-1-Policy-Network-pi-a-s-theta" class="headerlink" title="3.1 Policy Network $\pi(a|s;\theta)$"></a>3.1 Policy Network $\pi(a|s;\theta)$</h3><p>采用policy network $\pi(a|s;\theta)$来近似$\pi(a|s)$，其中$\theta$是神经网络的可训练参数。policy network接受当前的状态state s，通过神经网络的操作比如conv、dense，最后softmax得到结果。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061514836.png" style="zoom:33%;" /></p>
<p>注意</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061516405.png" style="zoom:33%;" /></p>
<h3 id="3-2-State-Value-Function-Approximation"><a href="#3-2-State-Value-Function-Approximation" class="headerlink" title="3.2 State-Value Function Approximation"></a>3.2 State-Value Function Approximation</h3><p>对于State Value Function $V_{\pi}(s)$，假如action是离散的，那么得到：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209042306730.png" style="zoom:33%;" /></p>
<p>将当中的$\pi(a|s)$用policy network $\pi(a|s;\theta)$代替，就得到了$V_{\pi}(s)$的近似$V(s_t;\theta)$：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061527296.png" style="zoom:33%;" /></p>
<p>那么，Policy-Based RL就是：学习参数$\theta$使得$J(\theta)$最大：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061529298.png" style="zoom:33%;" /></p>
<p>为啥要使得$J(\theta)$最大呢？先看下$J(\theta)$的定义，是状态价值函数$V$的关于随机变量$S$的期望。而状态价值函数用于评价当前状态的好坏，对其求期望可以用于评价所有状态下的一个平均好坏，那我们肯定希望平均状态更好一点，因此我们要maximize期望，也就是$J(\theta)$。</p>
<p>那么如何去学习到一个好的$\theta$能够使得$J(\theta)$最大呢？因为是要求最大，所以采用policy gradient ascent，也就是梯度上升。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061537725.png" style="zoom:33%;" /></p>
<p>这里的梯度其实应该是关于$J(\theta)$求的，但是这里写的是关于$V$求的，是因为这里采用随机梯度来代替求真正的梯度，而这种随机性来源于状态s。</p>
<p>问题来了，如何求$\frac{\partial V(s;\theta)}{\partial \theta}$ ？</p>
<p>看一个简单版本的推导，虽然过程不够严谨，但是结果是正确的，有助于理解。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061549717.png" style="zoom:33%;" /></p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061551577.png" style="zoom:33%;" /></p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061552967.png" style="zoom:33%;" /></p>
<p>所以最后的结果是：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061613535.png" style="zoom:33%;" /></p>
<p>要计算这个梯度，不管action是离散的还是连续的，可能都有点困难，所以可以采用蒙特卡洛，sample出来一个action $\hat{a}$，用$g(\hat{a},\theta)$来代替policy gradient。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061620972.png" style="zoom:33%;" /></p>
<p>使用policy gradient更新policy network的一次迭代：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061622646.png" style="zoom:33%;" /></p>
<h2 id="4-Actor-Critic-Methods"><a href="#4-Actor-Critic-Methods" class="headerlink" title="4. Actor-Critic Methods"></a>4. Actor-Critic Methods</h2><p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061716941.png" style="zoom:33%;" /></p>
<p>在Policy-Based RL的迭代过程中，$Q_{\pi}$是不知道的，如果在加上对$Q_{\pi}$的近似估计就得倒了AC。</p>
<h3 id="4-1-Value-Network-and-Policy-Network"><a href="#4-1-Value-Network-and-Policy-Network" class="headerlink" title="4.1 Value Network and Policy Network"></a>4.1 Value Network and Policy Network</h3><ul>
<li>Policy network (actor)：使用神经网络$\pi(a|s;\theta)$来近似$\pi(a|s)$</li>
</ul>
<p>输入是state s，输出是采取不同action的概率。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061724337.png" style="zoom:33%;" /></p>
<ul>
<li>Value network (critic)：使用神经网络$q(s,a;w)$来近似$Q_{\pi}(s,a)$</li>
</ul>
<p>输入是state s，输出是采取不同action的分数。</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061725517.png" style="zoom:33%;" /></p>
<h3 id="4-2-Train-the-Neural-Networks"><a href="#4-2-Train-the-Neural-Networks" class="headerlink" title="4.2 Train the Neural Networks"></a>4.2 Train the Neural Networks</h3><p>对于状态价值函数，如果将$\pi$和$q$都采用神经网络近似，就得到：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061727682.png" style="zoom:33%;" /></p>
<p>那么训练过程其实就是学习更新参数$\theta$和$w$。迭代过程如下：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061730007.png" style="zoom:33%;" /></p>
<p>下面说说具体如何使用不同算法更新参数。</p>
<ul>
<li>使用TD算法更新value network $q$</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061733735.png" style="zoom:33%;" /></p>
<ul>
<li>使用policy gradient更新policy network $\pi$</li>
</ul>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061738135.png" style="zoom:33%;" /></p>
<h3 id="4-3-Summary-of-Algorithm"><a href="#4-3-Summary-of-Algorithm" class="headerlink" title="4.3 Summary of Algorithm"></a>4.3 Summary of Algorithm</h3><p>在一次迭代中，算法流程如下：</p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202209061740479.png" style="zoom:33%;" /></p>
]]></content>
      <categories>
        <category>RL</category>
      </categories>
  </entry>
  <entry>
    <title>剑指offer-V2-03-数组中重复的数字</title>
    <url>/2022/10/27/%E5%89%91%E6%8C%87offer-v2-03-%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/</url>
    <content><![CDATA[<p><a href="https://leetcode.cn/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/?favorite=xb9nqhhg">原题点这里</a><br><span id="more"></span></p>
<p><img src="https://foursevenlove.oss-cn-hongkong.aliyuncs.com/pics/202210270948039.png" alt=""></p>
<h2 id="思路1"><a href="#思路1" class="headerlink" title="思路1"></a>思路1</h2><p>直接采用HashMap的思想：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">findRepeatNumber</span><span class="params">(nums []<span class="type">int</span>)</span></span> <span class="type">int</span> &#123;</span><br><span class="line">	exists := <span class="built_in">make</span>([]<span class="type">bool</span>, <span class="built_in">len</span>(nums))</span><br><span class="line">	<span class="keyword">for</span> _, num := <span class="keyword">range</span> nums &#123;</span><br><span class="line">		<span class="keyword">if</span> exists[num] &#123;</span><br><span class="line">			<span class="keyword">return</span> num</span><br><span class="line">		&#125;</span><br><span class="line">		exists[num] = <span class="literal">true</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="思路2"><a href="#思路2" class="headerlink" title="思路2"></a>思路2</h2><p>因为n个数字的的大小范围都在0～n-1之间，那么就可以把数字num1放到数组中下标为num1的地方。这样假设数字num1出现了重复，那么可以根据数组下标nums1位置上的数字是否等于数字num1来判断。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">findRepeatNumber</span><span class="params">(nums []<span class="type">int</span>)</span></span> <span class="type">int</span> &#123;</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(nums); &#123;</span><br><span class="line">		<span class="keyword">if</span> nums[i] == i &#123;</span><br><span class="line">			i++</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> nums[nums[i]] == nums[i] &#123;</span><br><span class="line">			<span class="keyword">return</span> nums[i]</span><br><span class="line">		&#125;</span><br><span class="line">		nums[nums[i]], nums[i] = nums[i], nums[nums[i]]</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>offer</tag>
      </tags>
  </entry>
</search>
